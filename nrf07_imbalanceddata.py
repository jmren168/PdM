# -*- coding: utf-8 -*-
"""NRF07_ImbalancedData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w8atwGAql8hdI1pjsR_jEFb1CIEEO5QH
"""

!pip install git+https://github.com/scikit-learn-contrib/imbalanced-learn.git
import imblearn

# download dataset
!wget --no-check-certificate "https://drive.google.com/uc?export=download&id=1hpCceb4EOt6Exh5LYJqlPWJocUHyI0Bg" -O data.npz  
import numpy as np

data = np.load('data.npz')
X,y = data['data'],data['label']

# plot data distribution
import matplotlib.pyplot as plt

plt.figure(figsize=(8,8))
unique, counts = np.unique(y, return_counts=True)

plt.bar(unique,counts)
plt.title('Class Frequency')
plt.xticks([-1,1])
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.show()

"""# Use SVM as a classifier to evaluate the performance on imbalanced dataset"""

from sklearn.svm import SVC # SVM for classification
from sklearn.model_selection import train_test_split # split訓練及測試資料

# split dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=0) # 依2:1隨機切割出training set and test set

# fit data
clf = SVC(C=1.0, kernel='rbf') # C: 分錯類別的penalty; kernel: 轉換空間
clf.fit(X_train, y_train)

# outside prediction
y_pred_test = clf.predict(X_test)

# inside prediction
y_pred_train = clf.predict(X_train)

# calculate performance
acc_outside = sum((y_test - y_pred_test)==0) / len(y_test) * 100
acc_inside = sum((y_train - y_pred_train)==0) / len(y_train) * 100

print('SVM')
print("inside accuracy: {:.2f}%".format(acc_inside))
print("outside accuracy: {:.2f}%".format(acc_outside))

"""# Is **HIGH** accuracy always good?

# Let's see the confusion matrix of prediction result
"""

# ref: https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets

# show confusion matrix
from sklearn.metrics import confusion_matrix
from matplotlib import pyplot as plt

conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred_test)
print('Confusion matrix:\n', conf_mat)
labels = ['Class -1', 'Class  1']


fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)
fig.colorbar(cax)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.xlabel('Predicted')
plt.ylabel('Expected')
plt.show()

"""# Re-sampling
![替代文字](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png)

ref: https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets

#Oversampling Minority Class
"""

from imblearn.over_sampling import RandomOverSampler

clf = RandomOverSampler(random_state=0)
X_res, y_res = clf.fit_resample(X,y)

plt.figure(figsize=(8,8))
unique, counts = np.unique(y_res, return_counts=True)

plt.bar(unique,counts)
plt.title('Class Frequency After Oversampling Minority Class')
plt.xticks([-1,1])
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.show()

"""# We train SVM on the sampled dataset, and then use it to evaluate the original test set"""

from sklearn.svm import SVC # SVM for classification
from sklearn.model_selection import train_test_split # split訓練及測試資料

# split dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=0) # 依2:1隨機切割出training set and test set

# balanced training dataset
ROS = RandomOverSampler(random_state=0)
X_train_res, y_train_res = ROS.fit_resample(X_train,y_train)

# fit data
SVM = SVC(C=1.0, kernel='rbf') # C: 分錯類別的penalty; kernel: 轉換空間
SVM.fit(X_train_res, y_train_res)

# outside prediction
y_pred_test = SVM.predict(X_test)

# inside prediction
y_pred_train_res = SVM.predict(X_train_res)

# calculate performance
acc_outside = sum((y_test - y_pred_test)==0) / len(y_test) * 100
acc_inside = sum((y_train_res - y_pred_train_res)==0) / len(y_train_res) * 100

print('SVM')
print("inside accuracy: {:.2f}%".format(acc_inside))
print("outside accuracy: {:.2f}%".format(acc_outside))

"""#Show the confusion matrix again"""

conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred_test)
print('Confusion matrix:\n', conf_mat)

labels = ['Class -1', 'Class 1']
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)
fig.colorbar(cax)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.xlabel('Predicted')
plt.ylabel('Expected')
plt.show()

"""# Model-Level Imbalanced Data Processing: use SVM as an example"""

from sklearn.svm import SVC # SVM for classification
from sklearn.model_selection import train_test_split # split訓練及測試資料

# split dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=0) # 依2:1隨機切割出training set and test set

# fit data
SVM = SVC(C=1.0, kernel='rbf', class_weight='balanced') # C: 分錯類別的penalty; kernel: 轉換空間
SVM.fit(X_train, y_train)

# outside prediction
y_pred_test = SVM.predict(X_test)

# inside prediction
y_pred_train = SVM.predict(X_train)

# calculate performance
acc_outside = sum((y_test - y_pred_test)==0) / len(y_test) * 100
acc_inside = sum((y_train - y_pred_train)==0) / len(y_train) * 100

print('SVM')
print("inside accuracy: {:.2f}%".format(acc_inside))
print("outside accuracy: {:.2f}%".format(acc_outside))

"""#Show the confusion matrix of model-level imbalanced data processing"""

conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred_test)
print('Confusion matrix:\n', conf_mat)

labels = ['Class -1', 'Class 1']
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)
fig.colorbar(cax)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.xlabel('Predicted')
plt.ylabel('Expected')
plt.show()